{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andrew Carr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "Run a multiclass (softmax) logistic regression on the scikit-learn digits dataset with the same train-test split we have used in the past. Experiment with different regularization parameters and choose the best. Justify your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = datasets.load_digits()\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.data, data.target, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predition accuracy standard 0.9666666666666667\n",
      "predition accuracy regularized 0.9703703703703703\n"
     ]
    }
   ],
   "source": [
    "# c = 1 in this case\n",
    "clf_standard = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "clf_standard.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "clf = LogisticRegression(C=0.1,multi_class='multinomial', solver='lbfgs')\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "print(\"predition accuracy standard {}\".format(clf_standard.score(x_test, y_test)))\n",
    "print(\"predition accuracy regularized {}\".format(clf.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some experimentation I found that a regularize coefficient of 0.1 works best with the current set up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "Install Keras and tensorflow on your computer. For most of you this can be done in one line with `conda install keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "Load the full MNIST dataset with keras's pre-chosen train-test split using\n",
    "from `keras.datasets import mnist`\n",
    "`(X_train, y_train), (X_test, y_test) = mnist.load_data()`\n",
    "and flatten the images into a single vector\n",
    "`input_dim = 784 #28*28`\n",
    "`X_train = X_train.reshape(60000, input_dim)`\n",
    "`X_test = X_test.reshape(10000, input_dim)`\n",
    "You may also need to convert the data to floats (they come as ints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "input_dim = 784\n",
    "X_train = X_train.reshape(60000, input_dim).astype(np.float32)\n",
    "X_test = X_test.reshape(10000, input_dim).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4\n",
    "\n",
    "Construct the multi-class matrix from y\n",
    "`from keras.utils import np_utils\n",
    "Y = np_utils.to_categorical(y, nb_classes)`\n",
    "and build a softmax classifier\n",
    "`from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation\n",
    "output_dim = 10 # number of classes\n",
    "soft = Sequential()\n",
    "soft.add(Dense(output_dim, input_dim=input_dim, activation='softmax'))\n",
    "soft.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_classes = 10\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_dim = 10\n",
    "soft = Sequential()\n",
    "soft.add(Dense(output_dim, input_dim=input_dim, activation='softmax'))\n",
    "# note that adam performs significantly better than sgd. This is not surprising, but is an interesting result\n",
    "soft.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5\n",
    "\n",
    "Experiment with various parameters, including different batch sizes and numbers of epochs to find the combination that gives the best results on the MNIST data set:\n",
    "`soft.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=1, validation_data=(X_test, Y_test))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 2.5455 - acc: 0.8386 - val_loss: 2.5473 - val_acc: 0.8391\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 2.4683 - acc: 0.8438 - val_loss: 2.5239 - val_acc: 0.8402\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 2.4397 - acc: 0.8458 - val_loss: 2.5021 - val_acc: 0.8420\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 2.4207 - acc: 0.8468 - val_loss: 2.5275 - val_acc: 0.8393\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 2.4161 - acc: 0.8473 - val_loss: 2.5050 - val_acc: 0.8418\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 2.4027 - acc: 0.8479 - val_loss: 2.5020 - val_acc: 0.8412\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 2.3913 - acc: 0.8486 - val_loss: 2.5181 - val_acc: 0.8408\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 2.3796 - acc: 0.8497 - val_loss: 2.5207 - val_acc: 0.8400\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 2.3780 - acc: 0.8499 - val_loss: 2.5158 - val_acc: 0.8407\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 2.3659 - acc: 0.8505 - val_loss: 2.5013 - val_acc: 0.8419\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 2.3588 - acc: 0.8513 - val_loss: 2.5031 - val_acc: 0.8408\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 2.3596 - acc: 0.8511 - val_loss: 2.5073 - val_acc: 0.8406\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 2.3583 - acc: 0.8510 - val_loss: 2.4986 - val_acc: 0.8415\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 2.3608 - acc: 0.8511 - val_loss: 2.5155 - val_acc: 0.8410\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 2.3507 - acc: 0.8519 - val_loss: 2.4993 - val_acc: 0.8414\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 2.3439 - acc: 0.8523 - val_loss: 2.4920 - val_acc: 0.8420\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 2.3392 - acc: 0.8527 - val_loss: 2.4879 - val_acc: 0.8431\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 2.3402 - acc: 0.8524 - val_loss: 2.4805 - val_acc: 0.8431\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 2.3322 - acc: 0.8531 - val_loss: 2.4976 - val_acc: 0.8419\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 2.3291 - acc: 0.8533 - val_loss: 2.4986 - val_acc: 0.8419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x133d1e0b8>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# due to the large number of data points I decided that a larger batch size would be useful (since we have a more representative sample)\n",
    "soft.fit(X_train, Y_train, batch_size=2000, epochs=20, verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting that the accuracy isn't really that great. I bet if I ran it for significantly longer I would be able to get much higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6\n",
    "\n",
    "Identify a multi-class classification problem related to your final project, using your project data. Use a softmax regression and choose an appropriate regularization parameter and appropriate choices of other hyperparameters and training parameters. Clearly identify your final preferred model, and explain why you chose that over the other contenders. What conclusions can be drawn from your results about the original classification question you asked?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note this is not my entire dataset, but a representative and useful sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"platplus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = df['Role_Code']\n",
    "\n",
    "# we are trying to predict which role a champion fits into\n",
    "li = ['Win Percent','Minions Killed', 'Total Healing', 'Team Jungle CS','Kills', 'Assists', 'Deaths', 'Damage Dealt', 'Damage Taken']\n",
    "x = df[li]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(x,y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax accuracy 0.819672131147541\n",
      "one vs rest accuracy 0.9344262295081968\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=(10**-1),multi_class='multinomial', solver='lbfgs')\n",
    "clf.fit(train_x, train_y)\n",
    "\n",
    "clf_or = LogisticRegression(C=(10**-1))\n",
    "clf_or.fit(train_x, train_y)\n",
    "\n",
    "print(\"softmax accuracy {}\".format(clf.score(test_x, test_y)))\n",
    "print(\"one vs rest accuracy {}\".format(clf_or.score(test_x, test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results here are super interesting. The softmax classification is performing far worse than the one-vs-rest method used in the previous homework where I was able to get accuracy of ~93%. I believe this is because a single champion can perform multiple roles and so the hard boundary in softmax is a negative feature that results in worse accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
