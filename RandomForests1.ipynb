{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "Assume that $X_1, X_2, ...$ is a sequence of iid Bernoulli random variables, and $X_i$ has probability $p$ of success.  Assume that $Y_1,Y_2, ...$ is another sequence of iid Bernoulli random variables, and every $Y_i$ has probability $q$ of success. Prove that if $p > q$, then \n",
    "\n",
    "$$Prob(\\frac{\\sum_{i=1}^n X_i}{n} > \\frac{1}{2}) > Prob(\\frac{\\sum_{i=1}^n Y_i}{n}  > \\frac{1}{2})$$ \n",
    "\n",
    "for all $n$. Hint: write out a binomial expansion of the probabilities involved compare, term by term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let $X_i$ be distributed bernoulli with parameter $p_i$ (q) in the problem and $Y_i$ be distributed similarly with parameter $p_i+\\delta_i$ which is p above. So we are actually trying to show the reverse, but that's how it was done in office hours.\n",
    "\n",
    "We then define $X = \\sum_{i=1}^n X_i$ with $Y = \\sum_{i=1}^n Y_i$. \n",
    "\n",
    "We first define a new variable $Z_i \\sim Bern(\\frac{\\delta_i}{1-p_i})$ and we notice that with $Y_i = min(1, X_i + Z_i) we recover $Y_i \\sim Bern(p_i + \\delta_i)$. Since \n",
    "\n",
    "$$(1-p_i)\\frac{1-p_i\\delta_i}{1-p_i} = 1 - p_i - \\delta_i$$\n",
    "\n",
    "is the probability of drawing $Z_i$ to be zero, so the probability that $Z_i$ is one is given as $p_i + \\delta_i$ as desired. \n",
    "\n",
    "Then we notice that since $Y_i \\geq X_i$ that $\\{Y \\leq K\\} \\subset \\{X \\leq K \\}$ we have\n",
    "\n",
    "$$\n",
    "P(Y \\leq K) \\leq P(X \\leq K) \\\\\n",
    "1 - P(Y > K ) \\leq 1 - P(X > K) \\\\\n",
    "P(Y > K) \\geq P(X > K) \\\\\n",
    "P(\\sum_{i=1}^n Y_i > \\frac{n}{2}) \\geq P(\\sum_{i=1}^n X_i > \\frac{n}{2}) \\\\\n",
    "P(\\frac{\\sum_{i=1}^n Y_i}{n} > \\frac{1}{2}) \\geq P(\\frac{\\sum_{i=1}^n X_i}{n} > \\frac{1}{2}) \\\\\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "Assume that $X_i$ are iid Bernoulli random variables with probability $p â‰¥ \\frac{2}{3}$ of success.  Use Cramer's theorem to give a lower bound on the number $n$ needed to give 95% confidence that $\\sum_{i=1}^n \\frac{X_i}{n} > \\frac{1}{2}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that cramer's theorem is given as follows\n",
    "\n",
    "$$\n",
    "P\\left( \\left| \\frac{\\sum_i^n X_i}{n} - p \\right| > \\epsilon \\right) < 2e^{-2 \\epsilon^2n}\n",
    "$$\n",
    "\n",
    "Using the derivation in class, we know that this implies\n",
    "\n",
    "$$\n",
    "P \\left ( \\frac{\\sum_i^n X_i}{n} > \\frac{1}{2} \\right ) \\geq 1 - 2 e^{-2 a^2 n}\n",
    "$$\n",
    "where $\\frac{1}{2} + a = \\frac{2}{3} \\implies a = \\frac{1}{6}$\n",
    "\n",
    "\n",
    "Therefore, we need to find for what $n$  \n",
    "\n",
    "$$\n",
    "1 - 2 e^{-2 \\frac{1}{6}^2 n} = .95\n",
    "$$\n",
    "\n",
    "$$\n",
    ".025 = e^{-2 \\frac{1}{6}^2 n}\n",
    "$$\n",
    "\n",
    "$$\n",
    "-18 log(.025) = n\n",
    "$$\n",
    "\n",
    "$$\n",
    "n \\approx 67\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.399830174050848"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-18*np.log(.025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "Use scikit learn's random forest classifier to predict survival for the titanic dataset (use an 80-20 train-test split). Experiment with `n_estimators` in range$(20,201,20)$ and `max_depth` in range$(2,10)$ and compare training time and prediction accuracy.  Also compare to the results you obtained last time with a singe tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best train time:\n",
      "Time: 0.02476191520690918\n",
      "Max Depth: 3\n",
      "n_estimators: 20\n",
      "\n",
      "best test time:\n",
      "Time: 0.0021190643310546875\n",
      "Max Depth: 2\n",
      "n_estimators: 20\n",
      "Accuracy: 0.746411483254\n",
      "\n",
      "best test accuracy:\n",
      "Time: 0.0036940574645996094\n",
      "Max Depth: 4\n",
      "n_estimators: 40\n",
      "Accuracy: 0.813397129187\n",
      "\n",
      "best 3 depth accuracy:\n",
      "Time: 0.01976776123046875\n",
      "Max Depth: 3\n",
      "n_estimators: 200\n",
      "Accuracy: 0.789473684211\n"
     ]
    }
   ],
   "source": [
    "# Import dataset.\n",
    "df = pd.read_csv(\"titanic.csv\")\n",
    "\n",
    "# Clean Data\n",
    "df.drop(['Body'],axis=1, inplace=True)\n",
    "df.drop(['Boat'],axis=1, inplace=True)\n",
    "df.drop(['home.dest'],axis=1, inplace=True)\n",
    "df.drop(['Cabin'],axis=1, inplace=True)\n",
    "df.dropna(axis=0,inplace=True)\n",
    "\n",
    "df['Sex'] = df['Sex'].astype(\"category\").cat.codes\n",
    "df['Embarked'] = df['Embarked'].astype(\"category\").cat.codes\n",
    "\n",
    "\n",
    "# Test train split.\n",
    "train_x, test_x, train_y, test_y = train_test_split(df[['Pclass','Sex','Age', 'Fare']], df['Survived'], train_size=.8, test_size=0.2)\n",
    "\n",
    "# Experiment with max_depth and min_samples_leaf.\n",
    "base_info = {\"accuracy\":0, \"time\":0, \"max_depth\":0, \"min_samples_leaf\":0, \"method\": \"none\"}\n",
    "meta_data = []\n",
    "\n",
    "# grid search\n",
    "for n_estimators in range(20,201,20):\n",
    "    for max_depth in range(2,10):\n",
    "        clf = RandomForestClassifier(max_depth=max_depth, n_estimators=n_estimators)\n",
    "        train_d = base_info.copy()\n",
    "        test_d = base_info.copy()\n",
    "\n",
    "        # save train meta data\n",
    "        start = time.time()\n",
    "        clf.fit(train_x, train_y)\n",
    "        train_d['time'] = time.time() - start\n",
    "        train_d['max_depth'] = max_depth\n",
    "        train_d['n_estimators'] = n_estimators\n",
    "        train_d['method'] = 'train'\n",
    "        \n",
    "        # save test meta data\n",
    "        start = time.time()\n",
    "        acc = clf.score(test_x, test_y)\n",
    "        test_d['accuracy'] = acc\n",
    "        test_d['time'] = time.time() - start\n",
    "        test_d['max_depth'] = max_depth\n",
    "        test_d['n_estimators'] = n_estimators\n",
    "        test_d['method'] = 'test'\n",
    "        meta_data.append(train_d)\n",
    "        meta_data.append(test_d)\n",
    "        \n",
    "# print results of min time and max accuracy across methods\n",
    "best_train_time = min([i for i in meta_data if i['method'] == 'train'], key=lambda x:x['time'])\n",
    "print(\"\\nbest train time:\",\n",
    "      \"\".join([\"Time: \"+ str(best_train_time['time']) +\"\\n\",\n",
    "               \"Max Depth: \"+ str(best_train_time['max_depth']) +\"\\n\",\n",
    "               \"n_estimators: \"+ str(best_train_time['n_estimators'])])\n",
    "      , sep=\"\\n\")\n",
    "\n",
    "best_test_time = min([i for i in meta_data if i['method'] == 'test'], key=lambda x:x['time'])\n",
    "print(\"\\nbest test time:\",\n",
    "      \"\".join([\"Time: \"+ str(best_test_time['time']) +\"\\n\",\n",
    "               \"Max Depth: \"+ str(best_test_time['max_depth']) +\"\\n\",\n",
    "               \"n_estimators: \"+ str(best_test_time['n_estimators']) + \"\\n\",\n",
    "               \"Accuracy: \"+ str(best_test_time['accuracy'])])\n",
    "      , sep=\"\\n\")\n",
    "\n",
    "best_test_accuracy = max([i for i in meta_data if i['method'] == 'test'], key=lambda x:x['accuracy'])\n",
    "print(\"\\nbest test accuracy:\",\n",
    "      \"\".join([\"Time: \"+ str(best_test_accuracy['time']) +\"\\n\",\n",
    "               \"Max Depth: \"+ str(best_test_accuracy['max_depth']) +\"\\n\",\n",
    "               \"n_estimators: \"+ str(best_test_accuracy['n_estimators']) + \"\\n\",\n",
    "               \"Accuracy: \"+ str(best_test_accuracy['accuracy'])])\n",
    "      , sep=\"\\n\")\n",
    "\n",
    "\n",
    "best_3_depth = max([i for i in meta_data if i['method'] == 'test' and i['max_depth'] == 3], key=lambda x:x['accuracy'])\n",
    "print(\"\\nbest 3 depth accuracy:\",\n",
    "      \"\".join([\"Time: \"+ str(best_3_depth['time']) +\"\\n\",\n",
    "               \"Max Depth: \"+ str(best_3_depth['max_depth']) +\"\\n\",\n",
    "               \"n_estimators: \"+ str(best_3_depth['n_estimators']) + \"\\n\",\n",
    "               \"Accuracy: \"+ str(best_3_depth['accuracy'])])\n",
    "      , sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Tree Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    best train time:\n",
    "    Time: 0.0004961490631103516\n",
    "    Max Depth: 2\n",
    "    Min Samples Leaf: 71\n",
    "\n",
    "    best test time:\n",
    "    Time: 0.00032591819763183594\n",
    "    Max Depth: 2\n",
    "    Min Samples Leaf: 61\n",
    "    Accuracy: 0.789473684211\n",
    "\n",
    "    best test accuracy:\n",
    "    Time: 0.00033164024353027344\n",
    "    Max Depth: 3\n",
    "    Min Samples Leaf: 1\n",
    "    Accuracy: 0.808612440191\n",
    "\n",
    "    best 3 depth accuracy:\n",
    "    Time: 0.00033164024353027344\n",
    "    Max Depth: 3\n",
    "    Min Samples Leaf: 1\n",
    "    Accuracy: 0.808612440191"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was expected, the random forest was much more accurate on this dataset. However, it is interesting to note that the random forests were much slower than the single tree. Which, again, is what was expected. It still is interesting to see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4\n",
    "Do the same thing as #3, but on a large dataset related to your final project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best train time:\n",
      "Time: 0.06557917594909668\n",
      "Max Depth: 2\n",
      "n_estimators: 20\n",
      "\n",
      "best test time:\n",
      "Time: 0.004752159118652344\n",
      "Max Depth: 2\n",
      "n_estimators: 20\n",
      "Accuracy: 0.412346842601\n",
      "\n",
      "best test accuracy:\n",
      "Time: 0.024680376052856445\n",
      "Max Depth: 9\n",
      "n_estimators: 80\n",
      "Accuracy: 0.453345900094\n",
      "\n",
      "best 3 depth accuracy:\n",
      "Time: 0.005567073822021484\n",
      "Max Depth: 3\n",
      "n_estimators: 20\n",
      "Accuracy: 0.419886899152\n"
     ]
    }
   ],
   "source": [
    "# Import dataset.\n",
    "df = pd.read_pickle(\"recipe_data\")\n",
    "\n",
    "# Test train split.\n",
    "train_x, test_x, train_y, test_y = train_test_split(df[['calories','fat', 'protein', 'sodium']], df['rating_floor'], train_size=.8, test_size=0.2)\n",
    "\n",
    "# Experiment with max_depth and min_samples_leaf.\n",
    "base_info = {\"accuracy\":0, \"time\":0, \"max_depth\":0, \"min_samples_leaf\":0, \"method\": \"none\"}\n",
    "meta_data = []\n",
    "\n",
    "#grid search\n",
    "for n_estimators in range(20,201,20):\n",
    "    for max_depth in range(2,10):\n",
    "        clf = RandomForestClassifier(max_depth=max_depth, n_estimators=n_estimators)\n",
    "        train_d = base_info.copy()\n",
    "        test_d = base_info.copy()\n",
    "\n",
    "        # save train meta data\n",
    "        start = time.time()\n",
    "        clf.fit(train_x, train_y)\n",
    "        train_d['time'] = time.time() - start\n",
    "        train_d['max_depth'] = max_depth\n",
    "        train_d['n_estimators'] = n_estimators\n",
    "        train_d['method'] = 'train'\n",
    "        \n",
    "        # save test meta data\n",
    "        start = time.time()\n",
    "        acc = clf.score(test_x, test_y)\n",
    "        test_d['accuracy'] = acc\n",
    "        test_d['time'] = time.time() - start\n",
    "        test_d['max_depth'] = max_depth\n",
    "        test_d['n_estimators'] = n_estimators\n",
    "        test_d['method'] = 'test'\n",
    "        meta_data.append(train_d)\n",
    "        meta_data.append(test_d)\n",
    "        \n",
    "# once again, this is just a convoluted way to get the visualization while saving data so it's logged well\n",
    "best_train_time = min([i for i in meta_data if i['method'] == 'train'], key=lambda x:x['time'])\n",
    "print(\"\\nbest train time:\",\n",
    "      \"\".join([\"Time: \"+ str(best_train_time['time']) +\"\\n\",\n",
    "               \"Max Depth: \"+ str(best_train_time['max_depth']) +\"\\n\",\n",
    "               \"n_estimators: \"+ str(best_train_time['n_estimators'])])\n",
    "      , sep=\"\\n\")\n",
    "\n",
    "best_test_time = min([i for i in meta_data if i['method'] == 'test'], key=lambda x:x['time'])\n",
    "print(\"\\nbest test time:\",\n",
    "      \"\".join([\"Time: \"+ str(best_test_time['time']) +\"\\n\",\n",
    "               \"Max Depth: \"+ str(best_test_time['max_depth']) +\"\\n\",\n",
    "               \"n_estimators: \"+ str(best_test_time['n_estimators']) + \"\\n\",\n",
    "               \"Accuracy: \"+ str(best_test_time['accuracy'])])\n",
    "      , sep=\"\\n\")\n",
    "\n",
    "best_test_accuracy = max([i for i in meta_data if i['method'] == 'test'], key=lambda x:x['accuracy'])\n",
    "print(\"\\nbest test accuracy:\",\n",
    "      \"\".join([\"Time: \"+ str(best_test_accuracy['time']) +\"\\n\",\n",
    "               \"Max Depth: \"+ str(best_test_accuracy['max_depth']) +\"\\n\",\n",
    "               \"n_estimators: \"+ str(best_test_accuracy['n_estimators']) + \"\\n\",\n",
    "               \"Accuracy: \"+ str(best_test_accuracy['accuracy'])])\n",
    "      , sep=\"\\n\")\n",
    "\n",
    "\n",
    "best_3_depth = max([i for i in meta_data if i['method'] == 'test' and i['max_depth'] == 3], key=lambda x:x['accuracy'])\n",
    "print(\"\\nbest 3 depth accuracy:\",\n",
    "      \"\".join([\"Time: \"+ str(best_3_depth['time']) +\"\\n\",\n",
    "               \"Max Depth: \"+ str(best_3_depth['max_depth']) +\"\\n\",\n",
    "               \"n_estimators: \"+ str(best_3_depth['n_estimators']) + \"\\n\",\n",
    "               \"Accuracy: \"+ str(best_3_depth['accuracy'])])\n",
    "      , sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Tree on Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    best train time:\n",
    "    Time: 0.003938198089599609\n",
    "    Max Depth: 2\n",
    "    Min Samples Leaf: 61\n",
    "\n",
    "    best test time:\n",
    "    Time: 0.0004010200500488281\n",
    "    Max Depth: 2\n",
    "    Min Samples Leaf: 51\n",
    "    Accuracy: 0.419886899152\n",
    "\n",
    "    best test accuracy:\n",
    "    Time: 0.0007512569427490234\n",
    "    Max Depth: 16\n",
    "    Min Samples Leaf: 1\n",
    "    Accuracy: 0.434024505184\n",
    "\n",
    "    best 3 depth accuracy:\n",
    "    Time: 0.00045299530029296875\n",
    "    Max Depth: 3\n",
    "    Min Samples Leaf: 71\n",
    "    Accuracy: 0.420358152686"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we see that the random forests had better accuracy ,but much slower train and test times. This is interesting, it would be good in a production system after training was complete, but it is definitely dependent on compute resources ahead of time and may not be excellent for a system that constantly needed to be retrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
