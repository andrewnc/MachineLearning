{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andrew Carr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.array([(x,y) for x,y in zip(digits.data,digits.target)])\n",
    "train, test = train_test_split(data, train_size=0.7)\n",
    "train_x, train_y = list(train[:,0]), list(train[:,1])\n",
    "test_x, test_y = list(test[:,0]), list(test[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_droppable_features(reg, accuracy, coefs):\n",
    "    \"\"\"Determine which features can be dropped based on their coefficient value\"\"\"\n",
    "    # get the best k values\n",
    "    best_k = np.argmax(accuracy)\n",
    "\n",
    "    # determine the mean of the coefficient values\n",
    "    mean_coef = np.mean(coefs[best_k], axis=0)\n",
    "\n",
    "    # if any of the coefficients are 0, we know they can be dropped\n",
    "    features_to_drop = np.where(mean_coef==0)\n",
    "    print(\"With {} we can drop the following features {}\".format(reg, features_to_drop[0]))\n",
    "    return features_to_drop[0]\n",
    "    \n",
    "def run_l2():\n",
    "    \"\"\"Run L2 regularized logistic regression on the current test/train set in memory\"\"\"\n",
    "    coefs = []\n",
    "    accuracy = []\n",
    "    for k in range(-10,11):\n",
    "        c = 10**k\n",
    "\n",
    "        clf = LogisticRegression(C=c)\n",
    "        clf.fit(train_x, train_y)\n",
    "\n",
    "        coefs.append(clf.coef_)\n",
    "\n",
    "        acc = clf.score(test_x, test_y)\n",
    "        accuracy.append(acc)\n",
    "\n",
    "        print(\"Accuracy for k = {} is {}\".format(k,acc))\n",
    "\n",
    "    get_droppable_features(\"L2\", accuracy, coefs)\n",
    "    \n",
    "def run_l1():\n",
    "    \"\"\"Run L1 regularized logistic regression on the current test/train set in memory\"\"\"\n",
    "    coefs = []\n",
    "    accuracy = []\n",
    "    for k in range(-10,11):\n",
    "        c = 10**k\n",
    "\n",
    "        clf = LogisticRegression(penalty='l1',C=c)\n",
    "        clf.fit(train_x, train_y)\n",
    "\n",
    "        coefs.append(clf.coef_)\n",
    "\n",
    "        acc = clf.score(test_x, test_y)\n",
    "        accuracy.append(acc)\n",
    "\n",
    "        print(\"Accuracy for k = {} is {}\".format(k,acc))\n",
    "\n",
    "    get_droppable_features(\"L1\", accuracy, coefs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = -10 is 0.8648148148148148\n",
      "Accuracy for k = -9 is 0.8648148148148148\n",
      "Accuracy for k = -8 is 0.8648148148148148\n",
      "Accuracy for k = -7 is 0.8759259259259259\n",
      "Accuracy for k = -6 is 0.8777777777777778\n",
      "Accuracy for k = -5 is 0.8796296296296297\n",
      "Accuracy for k = -4 is 0.9166666666666666\n",
      "Accuracy for k = -3 is 0.9481481481481482\n",
      "Accuracy for k = -2 is 0.9666666666666667\n",
      "Accuracy for k = -1 is 0.9648148148148148\n",
      "Accuracy for k = 0 is 0.9537037037037037\n",
      "Accuracy for k = 1 is 0.9444444444444444\n",
      "Accuracy for k = 2 is 0.9351851851851852\n",
      "Accuracy for k = 3 is 0.9296296296296296\n",
      "Accuracy for k = 4 is 0.924074074074074\n",
      "Accuracy for k = 5 is 0.9259259259259259\n",
      "Accuracy for k = 6 is 0.9277777777777778\n",
      "Accuracy for k = 7 is 0.9259259259259259\n",
      "Accuracy for k = 8 is 0.924074074074074\n",
      "Accuracy for k = 9 is 0.9259259259259259\n",
      "Accuracy for k = 10 is 0.924074074074074\n",
      "With L2 we can drop the following features [ 0 24 32 39]\n"
     ]
    }
   ],
   "source": [
    "# L2 on the digits dataset\n",
    "run_l2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that k = -1,0 have the highest accuracy. Which is quite interesting because it means $\\lambda \\in [\\frac{1}{10}, 1]$ gives us good accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = -10 is 0.09814814814814815\n",
      "Accuracy for k = -9 is 0.09814814814814815\n",
      "Accuracy for k = -8 is 0.09814814814814815\n",
      "Accuracy for k = -7 is 0.09814814814814815\n",
      "Accuracy for k = -6 is 0.09814814814814815\n",
      "Accuracy for k = -5 is 0.09814814814814815\n",
      "Accuracy for k = -4 is 0.09814814814814815\n",
      "Accuracy for k = -3 is 0.7388888888888889\n",
      "Accuracy for k = -2 is 0.9333333333333333\n",
      "Accuracy for k = -1 is 0.9592592592592593\n",
      "Accuracy for k = 0 is 0.9555555555555556\n",
      "Accuracy for k = 1 is 0.9444444444444444\n",
      "Accuracy for k = 2 is 0.9296296296296296\n",
      "Accuracy for k = 3 is 0.937037037037037\n",
      "Accuracy for k = 4 is 0.9314814814814815\n",
      "Accuracy for k = 5 is 0.9296296296296296\n",
      "Accuracy for k = 6 is 0.9314814814814815\n",
      "Accuracy for k = 7 is 0.9296296296296296\n",
      "Accuracy for k = 8 is 0.9314814814814815\n",
      "Accuracy for k = 9 is 0.9314814814814815\n",
      "Accuracy for k = 10 is 0.9296296296296296\n",
      "With L1 we can drop the following features [ 0  1  7  8 15 16 23 24 31 32 39 40 47 48 55 56 57]\n"
     ]
    }
   ],
   "source": [
    "# L1 on the digits dataset\n",
    "run_l1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we notice that large negative values give terrible accuracy. This suggests that there are some independent features that can be dropped to increase sparsity which is seen above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very large values of k give us the term that is closest to unregularized values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also see that L1 allows us to drop more features than L2 since is promotes sparsity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem B (my data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've added some of the cleaning code, because I haven't saved it yet, I will.. don't worry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace(piece):\n",
    "    platplus[piece] = platplus[piece].str.replace(\"%\", \"\").astype(\"float\")\n",
    "    bronze[piece] = bronze[piece].str.replace(\"%\", \"\").astype(\"float\")\n",
    "    silver[piece] = silver[piece].str.replace(\"%\", \"\").astype(\"float\")\n",
    "    gold[piece] = gold[piece].str.replace(\"%\", \"\").astype(\"float\")\n",
    "    plat[piece] = plat[piece].str.replace(\"%\", \"\").astype(\"float\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "platplus = pd.read_html(\"platplus.html\")[0]\n",
    "bronze = pd.read_html(\"bronze.html\")[0]\n",
    "silver = pd.read_html(\"silver.html\")[0]\n",
    "gold = pd.read_html(\"gold.html\")[0]\n",
    "plat = pd.read_html(\"plat.html\")[0]\n",
    "\n",
    "\n",
    "# clean the data\n",
    "replace('Win Percent')\n",
    "replace('Ban Rate')\n",
    "replace('Play Percent')\n",
    "\n",
    "platplus.dropna(inplace=True)\n",
    "bronze.dropna(inplace=True)\n",
    "silver.dropna(inplace=True)\n",
    "gold.dropna(inplace=True)\n",
    "plat.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Champion</th>\n",
       "      <th>Role</th>\n",
       "      <th>Win Percent</th>\n",
       "      <th>Play Percent</th>\n",
       "      <th>Ban Rate</th>\n",
       "      <th>Playerbase Avg. Games</th>\n",
       "      <th>Kills</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Largest Killing Spree</th>\n",
       "      <th>Damage Dealt</th>\n",
       "      <th>Damage Taken</th>\n",
       "      <th>Total Healing</th>\n",
       "      <th>Minions Killed</th>\n",
       "      <th>Enemy Jungle CS</th>\n",
       "      <th>Team Jungle CS</th>\n",
       "      <th>Gold Earned</th>\n",
       "      <th>Role Position</th>\n",
       "      <th>Position Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>Malzahar</td>\n",
       "      <td>Middle</td>\n",
       "      <td>53.94</td>\n",
       "      <td>6.10</td>\n",
       "      <td>3.25</td>\n",
       "      <td>7.51</td>\n",
       "      <td>5.12</td>\n",
       "      <td>5.35</td>\n",
       "      <td>7.52</td>\n",
       "      <td>9</td>\n",
       "      <td>19860</td>\n",
       "      <td>15639</td>\n",
       "      <td>1068</td>\n",
       "      <td>191.4</td>\n",
       "      <td>1.93</td>\n",
       "      <td>8.45</td>\n",
       "      <td>11800</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>72</td>\n",
       "      <td>Karma</td>\n",
       "      <td>Middle</td>\n",
       "      <td>50.02</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.14</td>\n",
       "      <td>10.40</td>\n",
       "      <td>4.49</td>\n",
       "      <td>5.00</td>\n",
       "      <td>9.01</td>\n",
       "      <td>8</td>\n",
       "      <td>18092</td>\n",
       "      <td>16609</td>\n",
       "      <td>2589</td>\n",
       "      <td>155.2</td>\n",
       "      <td>1.02</td>\n",
       "      <td>5.43</td>\n",
       "      <td>10671</td>\n",
       "      <td>45</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>118</td>\n",
       "      <td>Orianna</td>\n",
       "      <td>Middle</td>\n",
       "      <td>50.47</td>\n",
       "      <td>7.22</td>\n",
       "      <td>0.19</td>\n",
       "      <td>6.83</td>\n",
       "      <td>5.37</td>\n",
       "      <td>5.23</td>\n",
       "      <td>8.57</td>\n",
       "      <td>9</td>\n",
       "      <td>18816</td>\n",
       "      <td>16050</td>\n",
       "      <td>1673</td>\n",
       "      <td>178.1</td>\n",
       "      <td>1.82</td>\n",
       "      <td>9.30</td>\n",
       "      <td>11531</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>185</td>\n",
       "      <td>Volibear</td>\n",
       "      <td>Top</td>\n",
       "      <td>47.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.03</td>\n",
       "      <td>5.46</td>\n",
       "      <td>5.55</td>\n",
       "      <td>6.06</td>\n",
       "      <td>7</td>\n",
       "      <td>15908</td>\n",
       "      <td>31812</td>\n",
       "      <td>2527</td>\n",
       "      <td>153.5</td>\n",
       "      <td>1.60</td>\n",
       "      <td>2.62</td>\n",
       "      <td>10955</td>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>115</td>\n",
       "      <td>Nunu</td>\n",
       "      <td>Support</td>\n",
       "      <td>39.11</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.02</td>\n",
       "      <td>7.14</td>\n",
       "      <td>2.71</td>\n",
       "      <td>6.96</td>\n",
       "      <td>11.43</td>\n",
       "      <td>6</td>\n",
       "      <td>10217</td>\n",
       "      <td>22988</td>\n",
       "      <td>4844</td>\n",
       "      <td>39.6</td>\n",
       "      <td>0.98</td>\n",
       "      <td>3.47</td>\n",
       "      <td>9348</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Rank  Champion     Role  Win Percent  Play Percent  Ban Rate  \\\n",
       "96     97  Malzahar   Middle        53.94          6.10      3.25   \n",
       "71     72     Karma   Middle        50.02          0.92      0.14   \n",
       "117   118   Orianna   Middle        50.47          7.22      0.19   \n",
       "184   185  Volibear      Top        47.34          0.38      0.02   \n",
       "114   115      Nunu  Support        39.11          0.18      0.02   \n",
       "\n",
       "     Playerbase Avg. Games  Kills  Deaths  Assists  Largest Killing Spree  \\\n",
       "96                    7.51   5.12    5.35     7.52                      9   \n",
       "71                   10.40   4.49    5.00     9.01                      8   \n",
       "117                   6.83   5.37    5.23     8.57                      9   \n",
       "184                   5.03   5.46    5.55     6.06                      7   \n",
       "114                   7.14   2.71    6.96    11.43                      6   \n",
       "\n",
       "     Damage Dealt  Damage Taken  Total Healing  Minions Killed  \\\n",
       "96          19860         15639           1068           191.4   \n",
       "71          18092         16609           2589           155.2   \n",
       "117         18816         16050           1673           178.1   \n",
       "184         15908         31812           2527           153.5   \n",
       "114         10217         22988           4844            39.6   \n",
       "\n",
       "     Enemy Jungle CS  Team Jungle CS  Gold Earned  Role Position  \\\n",
       "96              1.93            8.45        11800              4   \n",
       "71              1.02            5.43        10671             45   \n",
       "117             1.82            9.30        11531             19   \n",
       "184             1.60            2.62        10955             49   \n",
       "114             0.98            3.47         9348             36   \n",
       "\n",
       "     Position Change  \n",
       "96                 2  \n",
       "71                 8  \n",
       "117                6  \n",
       "184                4  \n",
       "114                2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "platplus.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "platplus['Role_Code'] = platplus['Role'].astype(\"category\").cat.codes\n",
    "bronze['Role_Code'] = bronze['Role'].astype(\"category\").cat.codes\n",
    "silver['Role_Code'] = silver['Role'].astype(\"category\").cat.codes\n",
    "gold['Role_Code'] = gold['Role'].astype(\"category\").cat.codes\n",
    "plat['Role_Code'] = plat['Role'].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for column role reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ADC\n",
      "1 Jungle\n",
      "2 Middle\n",
      "3 Support\n",
      "4 Top\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    print(i, platplus.loc[platplus['Role_Code']==i, 'Role'].unique()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "platplus.to_csv(\"platplus.csv\", index=False)\n",
    "bronze.to_csv(\"bronze.csv\", index=False)\n",
    "silver.to_csv(\"silver.csv\", index=False)\n",
    "gold.to_csv(\"gold.csv\", index=False)\n",
    "plat.to_csv(\"plat.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(platplus, train_size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Rank', 'Champion', 'Role', 'Win Percent', 'Play Percent', 'Ban Rate',\n",
       "       'Playerbase Avg. Games', 'Kills', 'Deaths', 'Assists',\n",
       "       'Largest Killing Spree', 'Damage Dealt', 'Damage Taken',\n",
       "       'Total Healing', 'Minions Killed', 'Enemy Jungle CS', 'Team Jungle CS',\n",
       "       'Gold Earned', 'Role Position', 'Position Change', 'Role_Code'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = train['Role_Code']\n",
    "\n",
    "li = ['Win Percent', 'Play Percent', 'Ban Rate',\n",
    "       'Playerbase Avg. Games', 'Kills', 'Deaths', 'Assists',\n",
    "       'Largest Killing Spree', 'Damage Dealt', 'Damage Taken',\n",
    "       'Total Healing', 'Minions Killed', 'Enemy Jungle CS', 'Team Jungle CS',\n",
    "       'Gold Earned']\n",
    "\n",
    "train_x = train[li]\n",
    "\n",
    "test_y = test['Role_Code']\n",
    "test_x = test[li]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = -10 is 0.32786885245901637\n",
      "Accuracy for k = -9 is 0.4262295081967213\n",
      "Accuracy for k = -8 is 0.5081967213114754\n",
      "Accuracy for k = -7 is 0.5245901639344263\n",
      "Accuracy for k = -6 is 0.5901639344262295\n",
      "Accuracy for k = -5 is 0.7540983606557377\n",
      "Accuracy for k = -4 is 0.8524590163934426\n",
      "Accuracy for k = -3 is 0.8688524590163934\n",
      "Accuracy for k = -2 is 0.9180327868852459\n",
      "Accuracy for k = -1 is 0.9180327868852459\n",
      "Accuracy for k = 0 is 0.9016393442622951\n",
      "Accuracy for k = 1 is 0.9016393442622951\n",
      "Accuracy for k = 2 is 0.9180327868852459\n",
      "Accuracy for k = 3 is 0.9180327868852459\n",
      "Accuracy for k = 4 is 0.8852459016393442\n",
      "Accuracy for k = 5 is 0.9016393442622951\n",
      "Accuracy for k = 6 is 0.9016393442622951\n",
      "Accuracy for k = 7 is 0.9016393442622951\n",
      "Accuracy for k = 8 is 0.9180327868852459\n",
      "Accuracy for k = 9 is 0.9344262295081968\n",
      "Accuracy for k = 10 is 0.9016393442622951\n",
      "With L2 we can drop the following features []\n"
     ]
    }
   ],
   "source": [
    "run_l2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = -10 is 0.11475409836065574\n",
      "Accuracy for k = -9 is 0.11475409836065574\n",
      "Accuracy for k = -8 is 0.11475409836065574\n",
      "Accuracy for k = -7 is 0.11475409836065574\n",
      "Accuracy for k = -6 is 0.16393442622950818\n",
      "Accuracy for k = -5 is 0.36065573770491804\n",
      "Accuracy for k = -4 is 0.5081967213114754\n",
      "Accuracy for k = -3 is 0.639344262295082\n",
      "Accuracy for k = -2 is 0.8688524590163934\n",
      "Accuracy for k = -1 is 0.8688524590163934\n",
      "Accuracy for k = 0 is 0.9180327868852459\n",
      "Accuracy for k = 1 is 0.9344262295081968\n",
      "Accuracy for k = 2 is 0.9672131147540983\n",
      "Accuracy for k = 3 is 0.9672131147540983\n",
      "Accuracy for k = 4 is 0.9672131147540983\n",
      "Accuracy for k = 5 is 0.9672131147540983\n",
      "Accuracy for k = 6 is 0.9672131147540983\n",
      "Accuracy for k = 7 is 0.9672131147540983\n",
      "Accuracy for k = 8 is 0.9836065573770492\n",
      "Accuracy for k = 9 is 0.9672131147540983\n",
      "Accuracy for k = 10 is 0.9672131147540983\n",
      "With L1 we can drop the following features []\n"
     ]
    }
   ],
   "source": [
    "run_l1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with all of the features, we can still get 95% accuracy, which is great, but not consistently. Which is interesting. We really would like to reduce the number of features we want to use. But with my data, since I bet it's not linearly nicely behaved. However, using domain knowledge, I can choose the features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Smart Features Chosen Through OLS ETC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = train['Role_Code']\n",
    "li = ['Win Percent','Minions Killed', 'Total Healing', 'Team Jungle CS','Kills', 'Assists', 'Deaths', 'Damage Dealt', 'Damage Taken', 'Gold Earned']\n",
    "\n",
    "train_x = train[li]\n",
    "\n",
    "test_y = test['Role_Code']\n",
    "test_x = test[li]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = -10 is 0.32786885245901637\n",
      "Accuracy for k = -9 is 0.4262295081967213\n",
      "Accuracy for k = -8 is 0.5081967213114754\n",
      "Accuracy for k = -7 is 0.5245901639344263\n",
      "Accuracy for k = -6 is 0.5901639344262295\n",
      "Accuracy for k = -5 is 0.7540983606557377\n",
      "Accuracy for k = -4 is 0.8524590163934426\n",
      "Accuracy for k = -3 is 0.8688524590163934\n",
      "Accuracy for k = -2 is 0.9180327868852459\n",
      "Accuracy for k = -1 is 0.8852459016393442\n",
      "Accuracy for k = 0 is 0.8852459016393442\n",
      "Accuracy for k = 1 is 0.8688524590163934\n",
      "Accuracy for k = 2 is 0.8852459016393442\n",
      "Accuracy for k = 3 is 0.8688524590163934\n",
      "Accuracy for k = 4 is 0.8688524590163934\n",
      "Accuracy for k = 5 is 0.8688524590163934\n",
      "Accuracy for k = 6 is 0.8688524590163934\n",
      "Accuracy for k = 7 is 0.8688524590163934\n",
      "Accuracy for k = 8 is 0.8688524590163934\n",
      "Accuracy for k = 9 is 0.9016393442622951\n",
      "Accuracy for k = 10 is 0.8688524590163934\n",
      "With L2 we can drop the following features []\n"
     ]
    }
   ],
   "source": [
    "# predict champion roles based on certain critera\n",
    "run_l2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means I can predict role with ~95% accuracy! This is a huge thing because I am not using a very large number of features. Look at k=5. It depends on the train and test split, but this would definitely be my prefered method for choosing my C value. Since my dataset is so small, it is easy enough to do this since I don't have to worry too much about computational latency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = -10 is 0.11475409836065574\n",
      "Accuracy for k = -9 is 0.11475409836065574\n",
      "Accuracy for k = -8 is 0.11475409836065574\n",
      "Accuracy for k = -7 is 0.11475409836065574\n",
      "Accuracy for k = -6 is 0.16393442622950818\n",
      "Accuracy for k = -5 is 0.36065573770491804\n",
      "Accuracy for k = -4 is 0.5081967213114754\n",
      "Accuracy for k = -3 is 0.639344262295082\n",
      "Accuracy for k = -2 is 0.8688524590163934\n",
      "Accuracy for k = -1 is 0.8852459016393442\n",
      "Accuracy for k = 0 is 0.9180327868852459\n",
      "Accuracy for k = 1 is 0.8852459016393442\n",
      "Accuracy for k = 2 is 0.9016393442622951\n",
      "Accuracy for k = 3 is 0.8852459016393442\n",
      "Accuracy for k = 4 is 0.9180327868852459\n",
      "Accuracy for k = 5 is 0.9180327868852459\n",
      "Accuracy for k = 6 is 0.9016393442622951\n",
      "Accuracy for k = 7 is 0.8852459016393442\n",
      "Accuracy for k = 8 is 0.8852459016393442\n",
      "Accuracy for k = 9 is 0.8852459016393442\n",
      "Accuracy for k = 10 is 0.9180327868852459\n",
      "With L1 we can drop the following features []\n"
     ]
    }
   ],
   "source": [
    "run_l1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using this method of regularization we can get accuracy of ~95% which is also great. With k=0. It seems that L1 gives us better accuracy in general. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, regularization doesn't suggest we can drop features, but after working around (and using domain knowledge) it is obvious that there are improvements that can be made (not using all the features). So anyway, here ya go. Also I can't seem to get stats models to converge. I've tried everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = sm.MNLogit(train_y, train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-8b755aa3040a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMNLogit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_next_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'result= model.fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pinfo model.fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "model = sm.MNLogit(train_y, train_x)\n",
    "result= model.fit?\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result= model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = sm.OLS(train_y, train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(result.aic, result.bic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I prefer L1 regularized LogisticRegression for my personal data. This is because regardless of the regularization term, it seems to perform very well. It is also odd that if you look at the OLS version, it suggests that Gold Earned is not statistically significant, but if we don't include this feature in the analysis the accuracy on both L1 and L2 goes down (not shown).  However, we have been able to use the OLS to see and prune features. Even though both L1 and L2 don't suggest features to prune in the full model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
